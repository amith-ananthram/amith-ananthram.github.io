<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Amith Ananthram</title>

    <meta name="author" content="Amith Ananthram">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0">
        <td style="padding:0">
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;">
                  Amith Ananthram
                </p>
                <p>
                  I'm a PhD candidate in Computer Science at Columbia University, advised by Professor Kathy McKeown. My work explores vision-language models, in particular the strengths and limitations of different approaches to multimodal alignment.  Most recently, my focus has been on detailed image description with an emphasis on works of art.
                </p>
                <p>
                  Before returning to graduate school I built financial products at Stripe and Wealthfront as a full-stack software engineer.  I led cross-functional projects that delivered thoughtful experiences with rigorous technical solutions. I enjoy building reliable, maintainable systems that drive value for end users.
                </p>
                <p style="text-align:center">
                  <a href="mailto:amith@cs.columbia.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/AmithAnanthram-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/amith-ananthram-73065139/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=_dRPJYkAAAAJ&hl=en&oi=ao">Google Scholar</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%;vertical-align:middle">
                <div style="text-align:center">
                  <div style="width:100%;padding-top:100%;position:relative;border-radius:50%;border:2px dashed #ccc;display:block;">
                    <div style="position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);width:300px;height:300px;">
                      <img src="images/profile.jpg" alt="A headshot of Amith Ananthram" style="width:100%;height:100%;object-fit:cover;border-radius:50%;object-position:center 80%;">
                    </div>
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  Architectures, pre/post-training and evaluation methods for vision-language models.  Language and its role in vision.  
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  A complete list is available in my CV.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle;background-color:#ffffd0;">
                <span class="papertitle">PoSh: Using Scene Graphs to Guide LLMs-as-a-Judge for Detailed Image Descriptions</span>
                <br>
                <strong>Amith Ananthram</strong>, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown
                <br>
                <em>Under submission</em>, 2025
                <p>
                  <ul>
                    <li>Developed PoSh, an interpretable & replicable metric for detailed image descriptions.</li>
                    <li>Introduced DOCENT, a new dataset of artwork with expert descriptions and judgments from art history students.  DOCENT enables evaluating both detailed image description metrics and detailed image descriptions themselves.</li>
                    <li>Part of an ongoing collaboration with a team at the National Gallery of Art to expand accessibility in their collection.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2510.19060">paper</a></span> / <span><a href="https://github.com/amith-ananthram/posh">metric (PoSh)</a></span> / <span><a href="https://github.com/amith-ananthram/posh/tree/main/docent">datasets (DOCENT)</a></span> / <span><a href="https://huggingface.co/papers/2510.19060">huggingface (DOCENT)</a></span></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Mining Contextualized Visual Associations from Images for Creativity Understanding</span>
                <br>
                Ananya Sahu, <strong>Amith Ananthram</strong>, Kathleen McKeown
                <br>
                <em>INLG</em>, 2025 &nbsp; <font color="red"><strong>Best Long Paper</strong></font>
                <p>
                  We model how visual context shapes semantic associations to measure creativity in multimodal stories and artwork.
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2507.18915">paper</a></span> </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">See It from My Perspective: How Language Affects Cultural Bias in Image Understanding</span>
                <br>
                <strong>Amith Ananthram</strong>, Elias Stengel-Eskin, Mohit Bansal, Kathleen McKeown
                <br>
                <em>ICLR</em>, 2025
                <p>
                  Examines how linguistic framing shifts multimodal model judgments across cultures and proposes evaluation protocols that surface bias.
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2406.11665">paper</a></span> / <span><a href="https://github.com/amith-ananthram/see-it-from-my-perspective">code</a></span> / <span><a href="https://github.com/amith-ananthram/see-it-from-my-perspective/blob/main/iclr_poster.pdf">poster</a></span></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Data Caricatures: On the Representation of African American Language in Pretraining Corpora</span>
                <br>
                Nicholas Deas, Blake Vente, <strong>Amith Ananthram</strong>, Jessica A. Grieser, Desmond Patton, Shana Kleiner, James Shepard, Kathleen McKeown
                <br>
                <em>ACL</em>, 2025
                <p>
                  Analyzes how African American English is captured and distorted within large-scale language datasets to inform more equitable data practices.
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2503.10789">paper</a></span></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Enhancing Multimodal Affective Analysis with Learned Live Comment Features</span>
                <br>
                Zhaoyuan Deng, <strong>Amith Ananthram</strong>, Kathleen McKeown
                <br>
                <em>AAAI</em>, 2025
                <p>
                  Shows how incorporating live audience commentary can strengthen affective predictions in video understanding pipelines.
                </p>
                <p>Links: <span><a href="https://ojs.aaai.org/index.php/AAAI/article/view/33785">paper</a></span> </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">FeelingBlue: a Corpus for Understanding the Emotional Connotation of Color in Context</span>
                <br>
                <strong>Amith Ananthram</strong>, Olivia Winn, Smaranda Muresan
                <br>
                <em>TACL</em>, 2023 &nbsp; (Presented at ACL 2023)
                <p>
                  Introduces a large-scale dataset and modeling benchmarks for reasoning about color, emotion, and language jointly.
                </p>
                <p>Links: <span><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00540/115241/FeelingBlue-A-Corpus-for-Understanding-the">paper</a></span> / <span><a href="https://github.com/amith-ananthram/feelingblue">code</a></span> / <span><a href="https://huggingface.co/datasets/owinn/feelingblue_data">huggingface (FeelingBlue)</a></span> / <span><a href="https://github.com/amith-ananthram/feelingblue/blob/main/acl_2023_poster.pdf">poster</a></span></p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Industry Experience</h2>
                <p><strong>Stripe</strong>, Software Engineer (Levels 2–3), San Francisco, CA &nbsp; | &nbsp; Feb 2018 – Aug 2019</p>
                <p><strong>Wealthfront</strong>, Software Engineer (Levels 1–3), Redwood City, CA &nbsp; | &nbsp; Aug 2014 – Oct 2017</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Teaching</h2>
                <ul>
                  <li>Head TA, Language Generation Seminar (Columbia, Fall 2022)</li>
                  <li>Teaching Assistant, Natural Language Processing (Columbia, Fall 2021)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </tbody></table>
  </body>
</html>
