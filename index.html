<!DOCTYPE HTML>
<html lang="en">
  <script data-goatcounter="https://amithananthram.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Amith Ananthram</title>

    <meta name="author" content="Amith Ananthram">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0">
        <td style="padding:0">
          <table class="intro-table" style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0">
              <td class="intro-text" style="padding:2.5%;vertical-align:middle">
                <p class="name" style="text-align:center;">
                  Amith Ananthram
                </p>
                <p>
                  I'm a PhD candidate in Computer Science at Columbia University, advised by Professor Kathleen McKeown. My work explores vision-language models, in particular the strengths and limitations of language-mediated vision.  Most recently, my focus has been on detailed image description with an emphasis on works of art.
                </p>
                <p>
                  Before returning to graduate school I built financial products at Stripe and Wealthfront as a full-stack software engineer.  I led cross-functional projects that delivered thoughtful experiences with rigorous technical solutions. I enjoy building reliable, maintainable systems that drive value for end users.
                </p>
                <p style="text-align:center">
                  <a href="mailto:amith@cs.columbia.edu" data-goatcounter-click="email">Email</a> &nbsp;/&nbsp;
                  <a href="data/AmithAnanthram-CV.pdf" data-goatcounter-click="CV">CV</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/amith-ananthram-73065139/" data-goatcounter-click="linkedin">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=_dRPJYkAAAAJ&hl=en&oi=ao" data-goatcounter-click="gscholar">Google Scholar</a> &nbsp;/&nbsp; 
                  <a href="https://github.com/amith-ananthram" data-goatcounter-click="github">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://huggingface.co/amitha" data-goatcounter-click="huggingface">Hugging Face</a>
                </p>
              </td>
              <td class="intro-photo" style="padding:2.5%;vertical-align:middle">
                <div class="profile-wrapper">
                  <div class="profile-ring">
                    <img class="profile-image" src="images/profile.jpg" alt="A headshot of Amith Ananthram">
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  Architectures, pre/post-training, personalization and evaluation methods for image and video understanding in LLMs.  The strengths and limitations of language-mediated vision.
                </p>
              </td>
            </tr>
          </tbody></table>

          <div class="news-section">
            <h2>Recent News</h2>
            <div class="news-item">
              <span class="news-date">January 2026</span>
              <span class="news-content">Our paper <a href="https://arxiv.org/abs/2510.19060" data-goatcounter-click="news-posh-paper">"PoSh: Using Scene Graphs to Guide LLMs-as-a-Judge for Detailed Image Descriptions"</a> has been accepted at ICLR 2026!  I'll be at the conference in Brazil -- would love to chat about research and job opportunities!</span>
            </div>
          </div>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  A complete list is available in my CV.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle;background-color:#ffffd0;">
                <span class="papertitle">PoSh: Using Scene Graphs to Guide LLMs-as-a-Judge for Detailed Image Descriptions</span>
                <br>
                <strong>Amith Ananthram</strong>, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown
                <br>
                <em>ICLR</em>, 2026
                <p>
                  <ul>
                    <li>Developed PoSh, an interpretable & replicable metric for detailed image descriptions.</li>
                    <li>Introduced DOCENT, a new dataset of artwork with expert descriptions and judgments from art history students.  DOCENT enables evaluating both detailed image description metrics and detailed image descriptions themselves.</li>
                    <li>Part of an ongoing collaboration with a team at the National Gallery of Art to expand accessibility in their collection.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2510.19060" data-goatcounter-click="posh-paper">paper</a></span> / <span><a href="https://github.com/amith-ananthram/posh" data-goatcounter-click="posh-code">metric (PoSh)</a></span> / <span><a href="https://github.com/amith-ananthram/posh/tree/main/docent" data-goatcounter-click="posh-docent">datasets (DOCENT)</a></span> / <span><a href="https://huggingface.co/papers/2510.19060" data-goatcounter-click="posh-huggingface">huggingface (DOCENT)</a></span></p>
                <div class="paper-tags">
                  <span class="tag tag-datasets">datasets</span>
                  <span class="tag tag-post-training">post-training</span>
                  <span class="tag tag-evaluation">evaluation</span>
                  <span class="tag tag-images">images</span>
                </div>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Mining Contextualized Visual Associations from Images for Creativity Understanding</span>
                <br>
                Ananya Sahu, <strong>Amith Ananthram</strong>, Kathleen McKeown
                <br>
                <em>INLG</em>, 2025 &nbsp; <font color="red"><strong>Best Long Paper</strong></font>
                <p>
                  <ul>
                    <li>Developed a scalable method for mining contextualized visual associations from unlabeled images.</li>
                    <li>Demonstrated improved zero-shot performance in multimodal creative domains by fine-tuning on mined associations.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2507.18915" data-goatcounter-click="mining-paper">paper</a></span> </p>
                <div class="paper-tags">
                  <span class="tag tag-data-synthesis">data synthesis</span>
                  <span class="tag tag-pre-training">pre-training</span>
                  <span class="tag tag-images">images</span>
                </div>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">See It from My Perspective: How Language Affects Cultural Bias in Image Understanding</span>
                <br>
                <strong>Amith Ananthram</strong>, Elias Stengel-Eskin, Mohit Bansal, Kathleen McKeown
                <br>
                <em>ICLR</em>, 2025
                <p>
                  <ul>
                    <li>Characterized Western bias in vision-language models across visual tasks.</li>
                    <li>Identified language diversity in pre-training as a key factor in cultural bias, showing that inference in culturally-aligned languages reduces bias most effectively when those languages were well-represented during text-only pre-training.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2406.11665" data-goatcounter-click="sifmp-paper">paper</a></span> / <span><a href="https://github.com/amith-ananthram/see-it-from-my-perspective" data-goatcounter-click="sifmp-code">code</a></span> / <span><a href="https://github.com/amith-ananthram/see-it-from-my-perspective/blob/main/iclr_poster.pdf" data-goatcounter-click="sifmp-poster">poster</a></span></p>
                <div class="paper-tags">
                  <span class="tag tag-bias">bias</span>
                  <span class="tag tag-multicultural">multiculturalism</span>
                  <span class="tag tag-multilingualism">multilingualism</span>
                  <span class="tag tag-pre-training">pre-training</span>
                  <span class="tag tag-evaluation">evaluation</span>
                  <span class="tag tag-images">images</span>
                </div>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Data Caricatures: On the Representation of African American Language in Pretraining Corpora</span>
                <br>
                Nicholas Deas, Blake Vente, <strong>Amith Ananthram</strong>, Jessica A. Grieser, Desmond Patton, Shana Kleiner, James Shepard, Kathleen McKeown
                <br>
                <em>ACL</em>, 2025
                <p>
                  <ul>
                    <li>Revealed severe underrepresentation of African American Language (AAL) in pretraining corpora.</li>
                    <li>Demonstrated quality issues in AAL representation (harmful stereotypes) that are exacerbated by automated filters.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2503.10789" data-goatcounter-click="datacar-paper">paper</a></span></p>
                <div class="paper-tags">
                  <span class="tag tag-bias">bias</span>
                  <span class="tag tag-pre-training">pre-training</span>
                  <span class="tag tag-evaluation">evaluation</span>
                </div>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Enhancing Multimodal Affective Analysis with Learned Live Comment Features</span>
                <br>
                Zhaoyuan Deng, <strong>Amith Ananthram</strong>, Kathleen McKeown
                <br>
                <em>AAAI</em>, 2025
                <p>
                  <ul>
                    <li>Created the LCAffect dataset containing 11 million real-time comments for English and Chinese videos.</li>
                    <li>Developed a contrastive learning approach to generate synthetic live comment features from video encoders, achieving state-of-the-art performance on affective analysis in both English and Chinese.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://ojs.aaai.org/index.php/AAAI/article/view/33785" data-goatcounter-click="livecomments-paper">paper</a></span> </p>
                <div class="paper-tags">
                  <span class="tag tag-datasets">datasets</span>
                  <span class="tag tag-pre-training">pre-training</span>
                  <span class="tag tag-videos">videos</span>
                </div>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">FeelingBlue: a Corpus for Understanding the Emotional Connotation of Color in Context</span>
                <br>
                <strong>Amith Ananthram</strong>, Olivia Winn, Smaranda Muresan
                <br>
                <em>TACL</em>, 2023 &nbsp; (Presented at ACL 2023)
                <p>
                  <ul>
                    <li>Introduced FeelingBlue, a dataset with art annotated with emotion intensity and rationales of relative rankings.</li>
                    <li>Developed a neural ensemble model that recolors images to enhance specific emotions and justifies changes in text.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00540/115241/FeelingBlue-A-Corpus-for-Understanding-the" data-goatcounter-click="feelingblue-paper">paper</a></span> / <span><a href="https://github.com/amith-ananthram/feelingblue" data-goatcounter-click="feelingblue-code">code</a></span> / <span><a href="https://huggingface.co/datasets/owinn/feelingblue_data" data-goatcounter-click="feelingblue-data">huggingface (FeelingBlue)</a></span> / <span><a href="https://github.com/amith-ananthram/feelingblue/blob/main/acl_2023_poster.pdf" data-goatcounter-click="feelingblue-poster">poster</a></span></p>
                <div class="paper-tags">
                  <span class="tag tag-datasets">datasets</span>
                  <span class="tag tag-pre-training">pre-training</span>
                  <span class="tag tag-images">images</span>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Industry Experience</h2>
                <p><strong>Stripe</strong>, Software Engineer (Levels 2–3), San Francisco, CA &nbsp; | &nbsp; Feb 2018 – Aug 2019</p>
                <p><strong>Wealthfront</strong>, Software Engineer (Levels 1–3), Redwood City, CA &nbsp; | &nbsp; Aug 2014 – Oct 2017</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Teaching</h2>
                <ul>
                  <li>Head TA, Language Generation Seminar (Columbia, Fall 2022)</li>
                  <li>Teaching Assistant, Natural Language Processing (Columbia, Fall 2021)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </tbody></table>
  </body>
</html>
