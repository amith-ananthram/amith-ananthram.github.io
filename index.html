<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Amith Ananthram</title>

    <meta name="author" content="Amith Ananthram">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0">
        <td style="padding:0">
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;">
                  Amith Ananthram
                </p>
                <p>
                  I'm a PhD candidate in Computer Science at Columbia University, advised by Professor Kathleen McKeown. My work explores vision-language models, in particular the strengths and limitations of different approaches to multimodal alignment.  Most recently, my focus has been on detailed image description with an emphasis on works of art.
                </p>
                <p>
                  Before returning to graduate school I built financial products at Stripe and Wealthfront as a full-stack software engineer.  I led cross-functional projects that delivered thoughtful experiences with rigorous technical solutions. I enjoy building reliable, maintainable systems that drive value for end users.
                </p>
                <p style="text-align:center">
                  <a href="mailto:amith@cs.columbia.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/AmithAnanthram-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/amith-ananthram-73065139/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=_dRPJYkAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp; 
                  <a href="https://github.com/amith-ananthram">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://huggingface.co/amitha">Hugging Face</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%;vertical-align:middle">
                <div style="text-align:center">
                  <div style="width:100%;padding-top:100%;position:relative;border-radius:50%;border:2px dashed #ccc;display:block;">
                    <div style="position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);width:300px;height:300px;">
                      <img src="images/profile.jpg" alt="A headshot of Amith Ananthram" style="width:100%;height:100%;object-fit:cover;border-radius:50%;object-position:center 80%;">
                    </div>
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  Architectures, pre/post-training and evaluation methods for vision-language models.  Language and its role in vision.  
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
                <p>
                  A complete list is available in my CV.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle;background-color:#ffffd0;">
                <span class="papertitle">PoSh: Using Scene Graphs to Guide LLMs-as-a-Judge for Detailed Image Descriptions</span>
                <br>
                <strong>Amith Ananthram</strong>, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown
                <br>
                <em>Under submission</em>, 2025
                <p>
                  <ul>
                    <li>Developed PoSh, an interpretable & replicable metric for detailed image descriptions.</li>
                    <li>Introduced DOCENT, a new dataset of artwork with expert descriptions and judgments from art history students.  DOCENT enables evaluating both detailed image description metrics and detailed image descriptions themselves.</li>
                    <li>Part of an ongoing collaboration with a team at the National Gallery of Art to expand accessibility in their collection.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2510.19060">paper</a></span> / <span><a href="https://github.com/amith-ananthram/posh">metric (PoSh)</a></span> / <span><a href="https://github.com/amith-ananthram/posh/tree/main/docent">datasets (DOCENT)</a></span> / <span><a href="https://huggingface.co/papers/2510.19060">huggingface (DOCENT)</a></span></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Mining Contextualized Visual Associations from Images for Creativity Understanding</span>
                <br>
                Ananya Sahu, <strong>Amith Ananthram</strong>, Kathleen McKeown
                <br>
                <em>INLG</em>, 2025 &nbsp; <font color="red"><strong>Best Long Paper</strong></font>
                <p>
                  <ul>
                    <li>Developed a scalable method for mining contextualized visual associations from unlabeled images.</li>
                    <li>Demonstrated improved zero-shot performance in multimodal creative domains by fine-tuning on mined associations.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2507.18915">paper</a></span> </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">See It from My Perspective: How Language Affects Cultural Bias in Image Understanding</span>
                <br>
                <strong>Amith Ananthram</strong>, Elias Stengel-Eskin, Mohit Bansal, Kathleen McKeown
                <br>
                <em>ICLR</em>, 2025
                <p>
                  <ul>
                    <li>Characterized Western bias in vision-language models across visual tasks.</li>
                    <li>Identified language diversity in pre-training as a key factor in cultural bias, showing that inference in culturally-aligned languages reduces bias most effectively when those languages were well-represented during text-only pre-training.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2406.11665">paper</a></span> / <span><a href="https://github.com/amith-ananthram/see-it-from-my-perspective">code</a></span> / <span><a href="https://github.com/amith-ananthram/see-it-from-my-perspective/blob/main/iclr_poster.pdf">poster</a></span></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Data Caricatures: On the Representation of African American Language in Pretraining Corpora</span>
                <br>
                Nicholas Deas, Blake Vente, <strong>Amith Ananthram</strong>, Jessica A. Grieser, Desmond Patton, Shana Kleiner, James Shepard, Kathleen McKeown
                <br>
                <em>ACL</em>, 2025
                <p>
                  <ul>
                    <li>Revealed severe underrepresentation of African American Language (AAL) in pretraining corpora.</li>
                    <li>Demonstrated quality issues in AAL representation (harmful stereotypes) that are exacerbated by automated filters.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://arxiv.org/abs/2503.10789">paper</a></span></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">Enhancing Multimodal Affective Analysis with Learned Live Comment Features</span>
                <br>
                Zhaoyuan Deng, <strong>Amith Ananthram</strong>, Kathleen McKeown
                <br>
                <em>AAAI</em>, 2025
                <p>
                  <ul>
                    <li>Created the LCAffect dataset containing 11 million real-time comments for English and Chinese videos.</li>
                    <li>Developed a contrastive learning approach to generate synthetic live comment features from video encoders, achieving state-of-the-art performance on affective analysis in both English and Chinese.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://ojs.aaai.org/index.php/AAAI/article/view/33785">paper</a></span> </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <span class="papertitle">FeelingBlue: a Corpus for Understanding the Emotional Connotation of Color in Context</span>
                <br>
                <strong>Amith Ananthram</strong>, Olivia Winn, Smaranda Muresan
                <br>
                <em>TACL</em>, 2023 &nbsp; (Presented at ACL 2023)
                <p>
                  <ul>
                    <li>Introduced FeelingBlue, a dataset with art annotated with emotion intensity and rationales of relative rankings.</li>
                    <li>Developed a neural ensemble model that recolors images to enhance specific emotions and justifies changes in text.</li>
                  </ul>
                </p>
                <p>Links: <span><a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00540/115241/FeelingBlue-A-Corpus-for-Understanding-the">paper</a></span> / <span><a href="https://github.com/amith-ananthram/feelingblue">code</a></span> / <span><a href="https://huggingface.co/datasets/owinn/feelingblue_data">huggingface (FeelingBlue)</a></span> / <span><a href="https://github.com/amith-ananthram/feelingblue/blob/main/acl_2023_poster.pdf">poster</a></span></p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Industry Experience</h2>
                <p><strong>Stripe</strong>, Software Engineer (Levels 2–3), San Francisco, CA &nbsp; | &nbsp; Feb 2018 – Aug 2019</p>
                <p><strong>Wealthfront</strong>, Software Engineer (Levels 1–3), Redwood City, CA &nbsp; | &nbsp; Aug 2014 – Oct 2017</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Teaching</h2>
                <ul>
                  <li>Head TA, Language Generation Seminar (Columbia, Fall 2022)</li>
                  <li>Teaching Assistant, Natural Language Processing (Columbia, Fall 2021)</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </tbody></table>
  </body>
</html>
